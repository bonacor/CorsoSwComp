{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Project IRIS: with Keras+sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will discover **how to use Keras+sklearn to develop and evaluate a NN model for a multiclass classification problem**. \n",
    "\n",
    "Goals:\n",
    "* How to load data from CSV and make it available to Keras\n",
    "* How to prepare multiclass classification data for modeling with NNs\n",
    "* How to evaluate Keras NN models with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris flowers dataset is a standard ML dataset, widely used worldwide as benchmark.\n",
    "\n",
    "### Dataset availability:\n",
    "\n",
    "Almost ubiquitous.. e.g.\n",
    "   * [UCI Machine Learning repository](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)\n",
    "\n",
    "More info:\n",
    "   * [UCI Machine Learning Repository page](https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "Alternatively:\n",
    "   * get it from [https://github.com/bonacor/CorsoSwComp](https://github.com/bonacor/CorsoSwComp) by importing into from Google Colab\n",
    "      * direct URL to the dataset: [https://github.com/bonacor/CorsoSwComp/blob/master/iris.data.csv](https://github.com/bonacor/CorsoSwComp/blob/master/iris.data.csv)\n",
    "\n",
    "### Dataset description:\n",
    "\n",
    "* This is a good example to practice on a multiclass classification problem.\n",
    "* Each instance describes the properties of an observed iris flower measurements\n",
    "* All of the 4 input variables are numeric and have the same scale (cm)\n",
    "   * Sepal length in centimeters \n",
    "   * Sepal width in centimeters \n",
    "   * Petal length in centimeters \n",
    "   * Petal width in centimeters\n",
    "* The output variable is a specific iris species (3 possibilities)\n",
    "   * the \"class\", e.g. \"Iris-setosa\", \"Iris-versicolor\" or \"Iris-verginica\"\n",
    "\n",
    "### How the dataset looks like:\n",
    "\n",
    "\n",
    "    5.1,3.5,1.4,0.2,Iris-setosa\n",
    "    4.9,3.0,1.4,0.2,Iris-setosa\n",
    "    4.7,3.2,1.3,0.2,Iris-setosa\n",
    "    4.6,3.1,1.5,0.2,Iris-setosa\n",
    "    5.0,3.6,1.4,0.2,Iris-setosa\n",
    "    (...)\n",
    "\n",
    "\n",
    "### Additional input from best practitioners:\n",
    "\n",
    "The iris flower dataset is a well studied problem and as such we can expect to achieve a model accuracy in the range of 95% to 97%. USe this as target to aim for when developing your model(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Classes and Functions\n",
    "\n",
    "Start by importing all classes and functions you will need:\n",
    "* all the functionality we require from **Keras**\n",
    "* data loading functionalities from **Pandas**\n",
    "* data preparation and model evaluation from **scikit-learn**\n",
    "* more as needed (e.g. **numpy**, **matplotlib**, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# pandas\n",
    "from pandas import read_csv\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize random nb generator\n",
    "\n",
    "Important to ensure that the results we achieve from this model are repeatable, i.e. it ensures that the stochastic process of training a NN model can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Dataset\n",
    "\n",
    "Download the dataset from the link above and place it in your current working directory, with filename $iris.data.csv$. \n",
    "\n",
    "Then, you can quickly inspect it from this same ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-30 10:53:51--  https://raw.githubusercontent.com/bonacor/CorsoSwComp/master/iris.data.csv\n",
      "Resolving raw.githubusercontent.com... 151.101.240.133\n",
      "Connecting to raw.githubusercontent.com|151.101.240.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4551 (4.4K) [text/plain]\n",
      "Saving to: 'iris.data.csv.3'\n",
      "\n",
      "iris.data.csv.3     100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-05-30 10:53:51 (20.1 MB/s) - 'iris.data.csv.3' saved [4551/4551]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#today, get it from here for example:\n",
    "!wget https://raw.githubusercontent.com/bonacor/CorsoSwComp/master/iris.data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 bonacor  staff  4551 May 23 11:03 iris.data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -trl iris.data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\r\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\r\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\r\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\r\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 iris.data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the dataset. You can do it in various ways. The output variable contains strings, so it is suggested (easiest) to load the data using **pandas** into a DataFrame. While you do so, split the attributes (i.e. columns) into input variables (the matrix of **features X**) and output variables (the vector **label Y**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = read_csv(\"iris.data.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)   # columns from 1st to 4th into X\n",
    "Y = dataset[:,4]                   # column 5th into Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify what you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation/preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding the output variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(NOTE: different datasets may require different data manipulation/preprocessing. This applies to this specific case)*\n",
    "\n",
    "When modeling multiclass classification problems using NNs it is practice to reshape the output attribute from a vector that contains values for each class value to a matrix with a boolean for each class value and whether or not a given instance has that class value or not. \n",
    "\n",
    "In my case, the output variable contains 3 different class (string) values, i.e. I can have one of each of these 3 observations:\n",
    "\n",
    "    Iris-setosa\n",
    "    Iris-versicolor\n",
    "    Iris-virginica\n",
    "    \n",
    "I translate this into a one hot encoded binary matrix for each data instance among these 3:\n",
    "\n",
    "    Iris-setosa, Iris-versicolor, Iris-virginica\n",
    "\n",
    "\n",
    "that would hence look as follows:\n",
    "\n",
    "    1, 0, 0\n",
    "    0, 1, 0\n",
    "    0, 0, 1\n",
    "    \n",
    "We can do this easily by:\n",
    "   1. encoding the strings consistently to integers using the scikit-learn class *LabelEncoder*\n",
    "   2. convert the vector of integers to a one-hot encoding using the Keras function *to_categorical()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1: encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2: do one-hot encoding\n",
    "transformed_Y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baseline model\n",
    "\n",
    "You can create a baseline NN - a simple **Fully Connected NN (FCNN)** - for the IRIS multiclass classification problem with just one function:\n",
    "   * input\n",
    "       * as per our input dataset, this NN has 4 inputs (X)\n",
    "   * hidden layer(s)\n",
    "       * the hidden layer here has 8 nodes, and uses a rectifier (**relu**) activation function, which is a good practice\n",
    "   * output\n",
    "       * because we used a one-hot encoding for the dataset, the output layer must create 3 output values, one for each class. We use a **softmax** activation function in the output layer, to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities: the output value with the largest value will be taken as the class predicted by the model. Finally, the network uses the efficient **adam** GD optimization algorithm with a **logarithmic loss function**, which is called **categorical crossentropy** in Keras.   \n",
    "   \n",
    "Hence, the network topology of this simple 1-layer FCNN can be summarized as:\n",
    "\n",
    "    4 inputs -> 1 hidden layer with 8 nodes -> 3 outputs\n",
    "\n",
    "and it simplementation in Keras is as simple as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s - loss: 1.2464 - acc: 0.3333     \n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s - loss: 1.1843 - acc: 0.3333     \n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s - loss: 1.1170 - acc: 0.4533     \n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s - loss: 1.0669 - acc: 0.6200     \n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s - loss: 1.0332 - acc: 0.6600     \n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s - loss: 1.0088 - acc: 0.6600     \n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s - loss: 0.9881 - acc: 0.6600     \n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s - loss: 0.9697 - acc: 0.6600     \n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s - loss: 0.9508 - acc: 0.6600     \n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s - loss: 0.9319 - acc: 0.6600     \n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s - loss: 0.9130 - acc: 0.6600     \n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s - loss: 0.8939 - acc: 0.6600     \n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s - loss: 0.8748 - acc: 0.6600     \n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s - loss: 0.8557 - acc: 0.6600     \n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s - loss: 0.8369 - acc: 0.6600     \n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s - loss: 0.8182 - acc: 0.6667     \n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s - loss: 0.7988 - acc: 0.6667     \n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s - loss: 0.7802 - acc: 0.6667     \n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s - loss: 0.7625 - acc: 0.6667     \n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s - loss: 0.7445 - acc: 0.6667     \n",
      "Epoch 21/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.7307 - acc: 0.700 - 0s - loss: 0.7262 - acc: 0.6667     \n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s - loss: 0.7086 - acc: 0.6733     \n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s - loss: 0.6932 - acc: 0.6800     \n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s - loss: 0.6766 - acc: 0.6867     \n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s - loss: 0.6620 - acc: 0.6800     \n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s - loss: 0.6483 - acc: 0.6933     \n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s - loss: 0.6339 - acc: 0.7067     \n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s - loss: 0.6212 - acc: 0.7067     \n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s - loss: 0.6098 - acc: 0.7267     \n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s - loss: 0.5973 - acc: 0.7333     \n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s - loss: 0.5867 - acc: 0.7200     \n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s - loss: 0.5764 - acc: 0.7200     \n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s - loss: 0.5661 - acc: 0.7133     \n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s - loss: 0.5578 - acc: 0.7200     \n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s - loss: 0.5476 - acc: 0.7800     \n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s - loss: 0.5392 - acc: 0.8000     \n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s - loss: 0.5323 - acc: 0.8333     \n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s - loss: 0.5228 - acc: 0.8000     \n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s - loss: 0.5168 - acc: 0.7667     \n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s - loss: 0.5093 - acc: 0.7933     \n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s - loss: 0.5030 - acc: 0.7933     \n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s - loss: 0.4963 - acc: 0.8067     \n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s - loss: 0.4910 - acc: 0.8267     \n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s - loss: 0.4850 - acc: 0.8933     \n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s - loss: 0.4789 - acc: 0.8733     \n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s - loss: 0.4730 - acc: 0.8533     \n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s - loss: 0.4677 - acc: 0.8733     \n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s - loss: 0.4628 - acc: 0.8800     \n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s - loss: 0.4580 - acc: 0.8867     \n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s - loss: 0.4535 - acc: 0.8867     \n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s - loss: 0.4489 - acc: 0.9133     \n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s - loss: 0.4446 - acc: 0.8933     \n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s - loss: 0.4400 - acc: 0.8800     \n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s - loss: 0.4354 - acc: 0.9067     \n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s - loss: 0.4331 - acc: 0.9333     \n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s - loss: 0.4276 - acc: 0.9133     \n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s - loss: 0.4246 - acc: 0.9200     \n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s - loss: 0.4196 - acc: 0.9267     \n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s - loss: 0.4159 - acc: 0.9267     \n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s - loss: 0.4125 - acc: 0.9400     \n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s - loss: 0.4086 - acc: 0.9400     \n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s - loss: 0.4096 - acc: 0.9267     \n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s - loss: 0.4011 - acc: 0.9400     \n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s - loss: 0.3986 - acc: 0.9333     \n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s - loss: 0.3954 - acc: 0.9400     \n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s - loss: 0.3913 - acc: 0.9333     \n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s - loss: 0.3881 - acc: 0.9333     \n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s - loss: 0.3850 - acc: 0.9400     \n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s - loss: 0.3826 - acc: 0.9333     \n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s - loss: 0.3783 - acc: 0.9467     \n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s - loss: 0.3749 - acc: 0.9533     \n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s - loss: 0.3715 - acc: 0.9400     \n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s - loss: 0.3688 - acc: 0.9600     \n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s - loss: 0.3653 - acc: 0.9400     \n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s - loss: 0.3623 - acc: 0.9533     \n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s - loss: 0.3590 - acc: 0.9533     \n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s - loss: 0.3579 - acc: 0.9600     \n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s - loss: 0.3533 - acc: 0.9533     \n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s - loss: 0.3511 - acc: 0.9467     \n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s - loss: 0.3494 - acc: 0.9400     \n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s - loss: 0.3434 - acc: 0.9600     \n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s - loss: 0.3413 - acc: 0.9600     \n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s - loss: 0.3376 - acc: 0.9667     \n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s - loss: 0.3352 - acc: 0.9533     \n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s - loss: 0.3345 - acc: 0.9533     \n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s - loss: 0.3285 - acc: 0.9600     \n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s - loss: 0.3267 - acc: 0.9600     \n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s - loss: 0.3230 - acc: 0.9600     \n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s - loss: 0.3222 - acc: 0.9600     \n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s - loss: 0.3177 - acc: 0.9600     \n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s - loss: 0.3147 - acc: 0.9667     \n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s - loss: 0.3117 - acc: 0.9667     \n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s - loss: 0.3092 - acc: 0.9600     \n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s - loss: 0.3065 - acc: 0.9600     \n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s - loss: 0.3064 - acc: 0.9533     \n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s - loss: 0.3015 - acc: 0.9667     \n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s - loss: 0.2984 - acc: 0.9667     \n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s - loss: 0.2954 - acc: 0.9667     \n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s - loss: 0.2941 - acc: 0.9667     \n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s - loss: 0.2903 - acc: 0.9667     \n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "###\n",
    "#baseline_model()\n",
    "###\n",
    "#my_model = baseline_model()\n",
    "#0 history=model.fit(X, Y, epochs=10)\n",
    "#1 history=model.fit(X, transformed_Y, epochs=10)\n",
    "#2 history=model.fit(X, transformed_Y, epochs=100)\n",
    "#3 history=model.fit(X, transformed_Y, epochs=1000)\n",
    "#4 history=model.fit(X, transformed_Y, epochs=100, batch_size=32)\n",
    "#5 \n",
    "history=model.fit(X, transformed_Y, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c2eaadf90>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_variable=history.history[\"acc\"]\n",
    "plt.plot(range(len(my_variable)),my_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, transformed_Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s - loss: 1.4535 - acc: 0.3583 - val_loss: 1.4348 - val_acc: 0.2333\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s - loss: 1.2491 - acc: 0.3583 - val_loss: 1.2337 - val_acc: 0.2333\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s - loss: 1.0960 - acc: 0.3833 - val_loss: 1.0879 - val_acc: 0.5333\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s - loss: 0.9890 - acc: 0.6083 - val_loss: 0.9820 - val_acc: 0.6333\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s - loss: 0.9105 - acc: 0.6750 - val_loss: 0.9154 - val_acc: 0.6333\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s - loss: 0.8575 - acc: 0.6750 - val_loss: 0.8732 - val_acc: 0.5333\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s - loss: 0.8210 - acc: 0.8083 - val_loss: 0.8471 - val_acc: 0.6667\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s - loss: 0.7946 - acc: 0.7417 - val_loss: 0.8266 - val_acc: 0.6000\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s - loss: 0.7704 - acc: 0.6833 - val_loss: 0.8080 - val_acc: 0.6000\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s - loss: 0.7516 - acc: 0.6833 - val_loss: 0.7942 - val_acc: 0.6000\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s - loss: 0.7308 - acc: 0.6833 - val_loss: 0.7790 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s - loss: 0.7123 - acc: 0.6833 - val_loss: 0.7628 - val_acc: 0.6000\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s - loss: 0.6947 - acc: 0.6833 - val_loss: 0.7475 - val_acc: 0.6000\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s - loss: 0.6796 - acc: 0.6833 - val_loss: 0.7354 - val_acc: 0.6000\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s - loss: 0.6629 - acc: 0.6833 - val_loss: 0.7211 - val_acc: 0.6000\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s - loss: 0.6488 - acc: 0.6833 - val_loss: 0.7098 - val_acc: 0.6000\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s - loss: 0.6338 - acc: 0.6833 - val_loss: 0.6960 - val_acc: 0.6000\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s - loss: 0.6203 - acc: 0.7000 - val_loss: 0.6862 - val_acc: 0.6000\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s - loss: 0.6073 - acc: 0.7000 - val_loss: 0.6756 - val_acc: 0.6000\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s - loss: 0.5952 - acc: 0.7083 - val_loss: 0.6628 - val_acc: 0.6000\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5013 - acc: 0.800 - 0s - loss: 0.5832 - acc: 0.7167 - val_loss: 0.6538 - val_acc: 0.6000\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s - loss: 0.5725 - acc: 0.7417 - val_loss: 0.6420 - val_acc: 0.6333\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s - loss: 0.5606 - acc: 0.7583 - val_loss: 0.6348 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s - loss: 0.5510 - acc: 0.7667 - val_loss: 0.6240 - val_acc: 0.6667\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s - loss: 0.5407 - acc: 0.7667 - val_loss: 0.6176 - val_acc: 0.6333\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s - loss: 0.5304 - acc: 0.8000 - val_loss: 0.6085 - val_acc: 0.6667\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s - loss: 0.5210 - acc: 0.8333 - val_loss: 0.6010 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s - loss: 0.5121 - acc: 0.8417 - val_loss: 0.5919 - val_acc: 0.7000\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s - loss: 0.5029 - acc: 0.8417 - val_loss: 0.5837 - val_acc: 0.7000\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s - loss: 0.4940 - acc: 0.8417 - val_loss: 0.5755 - val_acc: 0.7000\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s - loss: 0.4864 - acc: 0.8500 - val_loss: 0.5691 - val_acc: 0.7000\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s - loss: 0.4768 - acc: 0.8583 - val_loss: 0.5605 - val_acc: 0.7333\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s - loss: 0.4679 - acc: 0.8917 - val_loss: 0.5519 - val_acc: 0.7667\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s - loss: 0.4600 - acc: 0.9000 - val_loss: 0.5437 - val_acc: 0.7667\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s - loss: 0.4529 - acc: 0.9000 - val_loss: 0.5373 - val_acc: 0.7667\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s - loss: 0.4466 - acc: 0.9333 - val_loss: 0.5278 - val_acc: 0.8000\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s - loss: 0.4385 - acc: 0.9333 - val_loss: 0.5232 - val_acc: 0.8000\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s - loss: 0.4323 - acc: 0.9333 - val_loss: 0.5177 - val_acc: 0.8000\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s - loss: 0.4266 - acc: 0.9333 - val_loss: 0.5135 - val_acc: 0.8000\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s - loss: 0.4208 - acc: 0.9333 - val_loss: 0.5082 - val_acc: 0.8000\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s - loss: 0.4164 - acc: 0.9500 - val_loss: 0.5023 - val_acc: 0.8333\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s - loss: 0.4106 - acc: 0.9417 - val_loss: 0.4990 - val_acc: 0.8333\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s - loss: 0.4057 - acc: 0.9500 - val_loss: 0.4935 - val_acc: 0.8333\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s - loss: 0.4001 - acc: 0.9500 - val_loss: 0.4898 - val_acc: 0.8333\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s - loss: 0.3956 - acc: 0.9583 - val_loss: 0.4848 - val_acc: 0.8333\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s - loss: 0.3914 - acc: 0.9750 - val_loss: 0.4793 - val_acc: 0.8333\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s - loss: 0.3856 - acc: 0.9750 - val_loss: 0.4767 - val_acc: 0.8333\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s - loss: 0.3825 - acc: 0.9583 - val_loss: 0.4726 - val_acc: 0.8333\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s - loss: 0.3770 - acc: 0.9750 - val_loss: 0.4684 - val_acc: 0.8333\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s - loss: 0.3727 - acc: 0.9583 - val_loss: 0.4666 - val_acc: 0.8333\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s - loss: 0.3688 - acc: 0.9583 - val_loss: 0.4613 - val_acc: 0.8333\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s - loss: 0.3639 - acc: 0.9750 - val_loss: 0.4576 - val_acc: 0.8333\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s - loss: 0.3597 - acc: 0.9750 - val_loss: 0.4532 - val_acc: 0.8667\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s - loss: 0.3566 - acc: 0.9750 - val_loss: 0.4515 - val_acc: 0.8333\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s - loss: 0.3525 - acc: 0.9667 - val_loss: 0.4468 - val_acc: 0.8667\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s - loss: 0.3474 - acc: 0.9833 - val_loss: 0.4423 - val_acc: 0.8667\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s - loss: 0.3438 - acc: 0.9833 - val_loss: 0.4382 - val_acc: 0.8667\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s - loss: 0.3406 - acc: 0.9833 - val_loss: 0.4339 - val_acc: 0.9000\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s - loss: 0.3363 - acc: 0.9833 - val_loss: 0.4318 - val_acc: 0.8667\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s - loss: 0.3332 - acc: 0.9833 - val_loss: 0.4283 - val_acc: 0.8667\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s - loss: 0.3292 - acc: 0.9833 - val_loss: 0.4265 - val_acc: 0.8667\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s - loss: 0.3256 - acc: 0.9833 - val_loss: 0.4218 - val_acc: 0.8667\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s - loss: 0.3225 - acc: 0.9833 - val_loss: 0.4208 - val_acc: 0.8667\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s - loss: 0.3187 - acc: 0.9833 - val_loss: 0.4173 - val_acc: 0.8667\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.3150 - acc: 0.9833 - val_loss: 0.4119 - val_acc: 0.9000\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s - loss: 0.3118 - acc: 0.9833 - val_loss: 0.4098 - val_acc: 0.9000\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s - loss: 0.3084 - acc: 0.9833 - val_loss: 0.4057 - val_acc: 0.9000\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s - loss: 0.3050 - acc: 0.9833 - val_loss: 0.4048 - val_acc: 0.8667\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s - loss: 0.3022 - acc: 0.9833 - val_loss: 0.4015 - val_acc: 0.8667\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s - loss: 0.2984 - acc: 0.9833 - val_loss: 0.3967 - val_acc: 0.9000\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s - loss: 0.2955 - acc: 0.9917 - val_loss: 0.3935 - val_acc: 0.9000\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s - loss: 0.2923 - acc: 0.9917 - val_loss: 0.3922 - val_acc: 0.9000\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s - loss: 0.2891 - acc: 0.9833 - val_loss: 0.3894 - val_acc: 0.9000\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s - loss: 0.2864 - acc: 0.9917 - val_loss: 0.3860 - val_acc: 0.9000\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s - loss: 0.2835 - acc: 0.9833 - val_loss: 0.3854 - val_acc: 0.8667\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s - loss: 0.2802 - acc: 0.9917 - val_loss: 0.3802 - val_acc: 0.9000\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s - loss: 0.2774 - acc: 0.9833 - val_loss: 0.3774 - val_acc: 0.9000\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s - loss: 0.2750 - acc: 0.9833 - val_loss: 0.3729 - val_acc: 0.8667\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s - loss: 0.2713 - acc: 0.9917 - val_loss: 0.3719 - val_acc: 0.9000\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s - loss: 0.2685 - acc: 0.9917 - val_loss: 0.3699 - val_acc: 0.9000\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s - loss: 0.2663 - acc: 0.9833 - val_loss: 0.3704 - val_acc: 0.9000\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s - loss: 0.2635 - acc: 0.9833 - val_loss: 0.3661 - val_acc: 0.9000\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s - loss: 0.2603 - acc: 0.9833 - val_loss: 0.3604 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s - loss: 0.2584 - acc: 0.9917 - val_loss: 0.3568 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s - loss: 0.2552 - acc: 0.9833 - val_loss: 0.3560 - val_acc: 0.8667\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s - loss: 0.2526 - acc: 0.9917 - val_loss: 0.3558 - val_acc: 0.9000\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s - loss: 0.2503 - acc: 0.9917 - val_loss: 0.3517 - val_acc: 0.8667\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s - loss: 0.2482 - acc: 0.9917 - val_loss: 0.3492 - val_acc: 0.8667\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s - loss: 0.2453 - acc: 0.9917 - val_loss: 0.3471 - val_acc: 0.8667\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s - loss: 0.2424 - acc: 0.9917 - val_loss: 0.3454 - val_acc: 0.9000\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s - loss: 0.2413 - acc: 0.9833 - val_loss: 0.3455 - val_acc: 0.9000\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s - loss: 0.2384 - acc: 0.9750 - val_loss: 0.3384 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s - loss: 0.2354 - acc: 0.9833 - val_loss: 0.3367 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s - loss: 0.2332 - acc: 0.9917 - val_loss: 0.3356 - val_acc: 0.9000\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s - loss: 0.2309 - acc: 0.9833 - val_loss: 0.3334 - val_acc: 0.9000\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s - loss: 0.2286 - acc: 0.9917 - val_loss: 0.3328 - val_acc: 0.9000\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s - loss: 0.2265 - acc: 0.9833 - val_loss: 0.3311 - val_acc: 0.9000\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s - loss: 0.2253 - acc: 0.9833 - val_loss: 0.3267 - val_acc: 0.9000\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s - loss: 0.2218 - acc: 0.9833 - val_loss: 0.3249 - val_acc: 0.9000\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s - loss: 0.2198 - acc: 0.9833 - val_loss: 0.3228 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "###\n",
    "#baseline_model()\n",
    "###\n",
    "#my_model = baseline_model()\n",
    "#0 history=model.fit(X_train, Y_train, epochs=10, validation_data=(X_test,Y_test))\n",
    "#1 history=model.fit(X_train, Y_train, epochs=10, validation_data=(X_test,Y_test))\n",
    "#2 history=model.fit(X_train, Y_train, epochs=100, validation_data=(X_test,Y_test))\n",
    "#3 history=model.fit(X_train, Y_train, epochs=1000, validation_data=(X_test,Y_test))\n",
    "#4 history=model.fit(X_train, Y_train, epochs=100, validation_data=(X_test,Y_test), batch_size=32)\n",
    "#5 \n",
    "history=model.fit(X_train, Y_train, epochs=100, validation_data=(X_test,Y_test), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c2f3d2ad0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "variable1=history.history[\"loss\"]\n",
    "variable2=history.history[\"val_loss\"]\n",
    "plt.plot(range(len(variable1)),variable1, label='loss')\n",
    "plt.plot(range(len(variable2)),variable2, label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (difficulty: easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same using only the last 100 entries of the dataset.\n",
    "   * Are you able to understand how the problem is modified by this choice, and which code changes are necessary to make this notebook work still?\n",
    "   * Are you able to interpret the results you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (difficulty: moderate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KerasClassifier\n",
    "\n",
    "\n",
    "The idea here is to use the Keras library which provides wrapper classes to allow you to use NN models developed with Keras (see cell above) in scikit-learn. Why so? Because Keras is simple, and scikit-learn is powerful and versatile!\n",
    "\n",
    "There is a *KerasClassifier* class in Keras that can be used as an *Estimator* in scikit-learn, the base type of model in the library. We need to actually create our KerasClassifier first, to be used in scikit-learn. KerasClassifier takes the name of a function (the one we wrote above) as an argument, plus arguments that will be passed on to the *fit()* function internally used to train the NN. Here, we pass:\n",
    "\n",
    "* a number of epochs as 200\n",
    "* a batch size as 5 \n",
    "\n",
    "to use when training the model. Debugging is also turned off when training by setting verbose to 0.\n",
    "    \n",
    "This function returns the constructed NN model, ready for training.\n",
    "\n",
    "Hint: https://keras.io/scikit-learn-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.wrappers.scikit_learn.KerasClassifier at 0x1c2f3e1cd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Model with $k$-Fold Cross-Validation\n",
    "\n",
    "It is time to evaluate our NN model on our training data, a.k.a. the \"training\" phase.\n",
    "\n",
    "The scikit-learn library has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating ML models is **k-fold cross-validation (k-fold CV)**. We do as follows:\n",
    "\n",
    "1. we define the model evaluation procedure.\n",
    "      * here, we shuffle the data before partitioning it, and we set the number of folds to 10 (a good default)\n",
    "     \n",
    "     \n",
    "2. we evaluate our model (*estimator*) on our dataset (*X* and *transformed_Y*) using a 10-fold CV procedure (kfold)\n",
    "\n",
    "Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the k=10 constructed models for each of the splits of the dataset. \n",
    "\n",
    "The results are summarized as both the mean and standard deviation of the model accuracy on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "# part 2\n",
    "results = cross_val_score(estimator, X, transformed_Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "What we learned:\n",
    "\n",
    "* how to develop and evaluate a NN using the Keras library for ML/DL.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* How to load data and make it available to Keras\n",
    "* How to prepare multiclass classification data for modeling using one hot encoding\n",
    "* How to use Keras NN models with scikit-learn\n",
    "* How to define a NN using Keras for multiclass classification\n",
    "* How to evaluate a Keras NN model using scikit-learn with k-fold cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
